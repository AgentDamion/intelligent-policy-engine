# ğŸ‰ GEPA + PolicyAgent Integration Complete!

## What's Been Implemented

You now have a **complete, end-to-end prompt optimization system** that:

1. âœ… **Optimizes prompts** using GPT-4 and real agent activity data
2. âœ… **Stores prompts** in Supabase (`agent_prompts_v2`)
3. âœ… **PolicyAgent loads prompts** from database automatically
4. âœ… **Caches prompts** for performance (5-minute TTL)
5. âœ… **Falls back** to hardcoded prompt if database is unavailable
6. âœ… **Tracks performance** metrics and improvements

---

## How It Works: The Complete Flow

### 1. Initial Setup (One-Time)

```bash
# Seed current PolicyAgent prompt (version 1)
cd packages/ai-optimization
npm run seed-prompt

# Generate 20 synthetic training examples
npm run generate-data
```

**What happens:**
- Current prompt is stored in `agent_prompts_v2` with `is_active = true`
- 20 realistic policy evaluation scenarios are inserted into `agent_activities`
- Each example includes human feedback for training

---

### 2. PolicyAgent Execution (Production)

When a user makes a policy evaluation request:

```
User Request
    â†“
PolicyAgent.process()
    â†“
getActivePrompt() â†’ Query agent_prompts_v2
    â†“
Load version 2 (optimized) or version 1 (baseline)
    â†“
Fill template with user input
    â†“
Send to OpenAI with optimized system prompt
    â†“
Return decision
```

**Key Features:**
- **Caching**: Prompts are cached for 5 minutes to reduce DB queries
- **Fallback**: If DB fails, uses hardcoded prompt (zero downtime)
- **Version tracking**: Logs which prompt version was used in metadata
- **Hot-swapping**: Activate new prompts without redeploying code

---

### 3. Optimization Cycle (Weekly/On-Demand)

```bash
# Run optimization (analyzes failures, generates improved prompt)
npm run test
```

**What happens:**

1. **Extract Training Data** - Gets 20 examples from `agent_activities`
2. **Evaluate Baseline** - Tests current prompt â†’ ~50-70% accuracy
3. **Find Failures** - Identifies where agent made wrong decisions
4. **GPT-4 Reflection** - Analyzes failure patterns and proposes improvements
5. **Generate Improved Prompt** - Creates version 2 with fixes
6. **Evaluate Improved** - Tests new prompt â†’ ~75-90% accuracy
7. **Save Results** - Stores everything in database for review

**Output:**
```
Baseline Score:    65.0%
Improved Score:    82.5%
Improvement:       +26.9%
Cost:              $0.70
New Prompt ID:     abc-123-def
```

---

### 4. Human Review & Activation

```sql
-- Step 1: Review the improved prompt
SELECT 
  system_prompt,
  user_prompt_template,
  improvement_percentage,
  performance_metrics
FROM agent_prompts_v2
WHERE prompt_version = 2;

-- Step 2: Read GPT-4's reasoning
SELECT diagnosis, key_issues, proposed_changes
FROM prompt_reflections_v2
WHERE resulting_prompt_id = 'abc-123-def';

-- Step 3: Activate if satisfied
SELECT activate_prompt_v2('abc-123-def', auth.uid());
```

**What happens:**
- Previous prompt (v1) is deactivated
- New prompt (v2) becomes active
- Change is logged in `audit_events`
- Within 5 minutes (cache TTL), ALL PolicyAgent requests use v2

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PRODUCTION FLOW                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Request                                               â”‚
â”‚    â†“                                                         â”‚
â”‚  Edge Function: PolicyAgent.process()                       â”‚
â”‚    â†“                                                         â”‚
â”‚  getActivePrompt()                                          â”‚
â”‚    â”œâ”€ Check cache (5min TTL)                                â”‚
â”‚    â”œâ”€ If expired â†’ Query agent_prompts_v2                   â”‚
â”‚    â””â”€ If DB fails â†’ Use hardcoded fallback                  â”‚
â”‚    â†“                                                         â”‚
â”‚  fillPromptTemplate(input)                                  â”‚
â”‚    â”œâ”€ Replace {tool}, {vendor}, {usage}, etc.               â”‚
â”‚    â””â”€ Create user message                                   â”‚
â”‚    â†“                                                         â”‚
â”‚  Call OpenAI                                                â”‚
â”‚    â”œâ”€ System: DB prompt (optimized)                         â”‚
â”‚    â””â”€ User: Filled template                                 â”‚
â”‚    â†“                                                         â”‚
â”‚  Return decision + log activity                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 OPTIMIZATION FLOW (OFFLINE)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Weekly Cron / On-Demand Trigger                            â”‚
â”‚    â†“                                                         â”‚
â”‚  PromptOptimizer.optimizeAgent()                            â”‚
â”‚    â†“                                                         â”‚
â”‚  Extract training data from agent_activities                â”‚
â”‚    â”œâ”€ Parse details.input, details.output                   â”‚
â”‚    â”œâ”€ Get details.metadata.human_review                     â”‚
â”‚    â””â”€ Calculate scores (match vs expected)                  â”‚
â”‚    â†“                                                         â”‚
â”‚  Evaluate baseline (current active prompt)                  â”‚
â”‚    â”œâ”€ Run on 10 test examples                               â”‚
â”‚    â””â”€ Calculate accuracy                                    â”‚
â”‚    â†“                                                         â”‚
â”‚  Identify failures (score < 0.7)                            â”‚
â”‚    â†“                                                         â”‚
â”‚  GPT-4 Reflection                                           â”‚
â”‚    â”œâ”€ Analyze failure patterns                              â”‚
â”‚    â”œâ”€ Identify root causes                                  â”‚
â”‚    â””â”€ Generate improved prompt                              â”‚
â”‚    â†“                                                         â”‚
â”‚  Evaluate improved prompt                                   â”‚
â”‚    â”œâ”€ Run on same 10 test examples                          â”‚
â”‚    â””â”€ Calculate new accuracy                                â”‚
â”‚    â†“                                                         â”‚
â”‚  Save results to DB                                         â”‚
â”‚    â”œâ”€ agent_prompts_v2 (new version, inactive)              â”‚
â”‚    â”œâ”€ optimization_runs_v2 (metrics)                        â”‚
â”‚    â””â”€ prompt_reflections_v2 (GPT-4 analysis)                â”‚
â”‚    â†“                                                         â”‚
â”‚  Human reviews and activates                                â”‚
â”‚    â†“                                                         â”‚
â”‚  activate_prompt_v2(new_id)                                 â”‚
â”‚    â”œâ”€ Deactivate current                                    â”‚
â”‚    â”œâ”€ Activate new                                          â”‚
â”‚    â””â”€ Log in audit_events                                   â”‚
â”‚    â†“                                                         â”‚
â”‚  Next PolicyAgent request uses optimized prompt!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Changes Made

### Modified Files

1. **`supabase/functions/cursor-agent-adapter/shared/ai-client.ts`**
   - Added `systemPrompt?` to `AIRequest` interface
   - AI client now accepts custom system prompts from database

2. **`supabase/functions/cursor-agent-adapter/agents/policy-agent.ts`**
   - Added Supabase client initialization
   - Added `getActivePrompt()` - loads prompt from `agent_prompts_v2`
   - Added `getFallbackPrompt()` - hardcoded fallback
   - Added `fillPromptTemplate()` - fills placeholders
   - Added 5-minute caching for performance
   - Now passes DB prompt to AI client

3. **All files in `packages/ai-optimization/`**
   - Updated to use v2 table names (`agent_prompts_v2`, etc.)

---

## Database Tables

### agent_prompts_v2
```sql
id                      UUID PRIMARY KEY
agent_type             TEXT (e.g., 'PolicyAgent')
prompt_version         INTEGER
system_prompt          TEXT (full system instruction)
user_prompt_template   TEXT (with {placeholders})
is_active              BOOLEAN (only one active per agent)
performance_metrics    JSONB (scores, improvements)
parent_prompt_id       UUID (version history)
improvement_percentage NUMERIC(5,2)
created_at             TIMESTAMPTZ
```

### optimization_runs_v2
```sql
id                        UUID PRIMARY KEY
agent_type                TEXT
status                    TEXT ('running', 'completed', 'failed')
baseline_score            NUMERIC(5,4)
improved_score            NUMERIC(5,4)
improvement_percentage    NUMERIC(5,2)
best_prompt_id            UUID â†’ agent_prompts_v2
started_at                TIMESTAMPTZ
completed_at              TIMESTAMPTZ
```

### prompt_reflections_v2
```sql
id                    UUID PRIMARY KEY
optimization_run_id   UUID â†’ optimization_runs_v2
parent_prompt_id      UUID â†’ agent_prompts_v2
diagnosis             TEXT (GPT-4's analysis)
key_issues            JSONB (array of problems)
proposed_changes      TEXT (what to fix)
reasoning             TEXT (why changes help)
resulting_prompt_id   UUID â†’ agent_prompts_v2
```

---

## Testing the Integration

### Test 1: Verify PolicyAgent Loads from DB

1. Seed the initial prompt:
```bash
cd packages/ai-optimization
npm run seed-prompt
```

2. Check Supabase:
```sql
SELECT * FROM agent_prompts_v2 WHERE agent_type = 'PolicyAgent';
-- Should see version 1, is_active = true
```

3. Make a PolicyAgent request (via your app or Edge function test)
4. Check logs - should see: `âœ… Using PolicyAgent prompt version 1`

### Test 2: Optimize and Activate

1. Generate training data:
```bash
npm run generate-data
```

2. Run optimization:
```bash
npm run test
```

3. You should see:
```
âœ… Using PolicyAgent prompt version 1
ğŸ“Š Found 20 examples
ğŸ“ˆ Baseline accuracy: 65.0%
ğŸ¤” Generating GPT-4 reflection...
ğŸ“ˆ Improved accuracy: 82.5%
âœ… Optimization complete! Improvement: 26.9%
```

4. Activate the new prompt:
```sql
-- Get the new prompt ID from test output
SELECT activate_prompt_v2('new-prompt-id-here', auth.uid());
```

5. Wait 5 minutes (or restart Edge functions)

6. Make another PolicyAgent request
7. Check logs - should see: `âœ… Using PolicyAgent prompt version 2`

### Test 3: Verify Fallback

1. Temporarily break the DB connection (wrong key in env)
2. Make a PolicyAgent request
3. Check logs - should see: `âš ï¸ Failed to load prompt from DB, using fallback`
4. Request should still work (uses hardcoded prompt)

---

## Monitoring & Metrics

### Track Optimization Effectiveness

```sql
SELECT 
  agent_type,
  prompt_version,
  started_at,
  baseline_score,
  improved_score,
  improvement_percentage
FROM optimization_runs_v2
ORDER BY started_at DESC;
```

### Track Production Usage by Version

```sql
-- Add to your agent_activities insert
metadata: {
  ...existingMetadata,
  prompt_version: promptData.prompt_version
}

-- Then query:
SELECT 
  details->'metadata'->>'prompt_version' as version,
  COUNT(*) as requests,
  AVG(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success_rate
FROM agent_activities
WHERE agent = 'PolicyAgent'
GROUP BY version;
```

---

## Costs

**Per Optimization Run**: ~$0.70
- Baseline evaluation: 10 GPT-4 calls @ $0.02 each
- GPT-4 reflection: 1 call @ $0.50
- Improved evaluation: 10 GPT-4 calls @ $0.02 each

**Production (per request)**: Same as before
- PolicyAgent caches prompts (5min)
- DB query only on cache miss (~1 per 5 minutes)
- No extra OpenAI cost

**Monthly (weekly optimization)**: ~$3-4/month

---

## Next Steps

1. **Collect Real Data**
   - Build a human review UI to capture actual feedback
   - Replace synthetic data with production decisions + reviews

2. **A/B Testing**
   - Run 50% traffic on v1, 50% on v2
   - Compare performance metrics
   - Activate winner

3. **Expand to Other Agents**
   - ComplianceScoringAgent
   - AuditAgent
   - ContextAgent
   - All use same optimization framework!

4. **Automate**
   - Weekly cron job to run optimization
   - Slack notification with results
   - Optional: Auto-activate if improvement > 10%

---

## Troubleshooting

### "No active prompt found"
â†’ Run `npm run seed-prompt`

### PolicyAgent still using old prompt
â†’ Wait 5 minutes for cache to expire, or restart Edge functions

### "Failed to load prompt from DB"
â†’ Check `SUPABASE_SERVICE_ROLE_KEY` is set in Edge function env
â†’ Verify RLS policies allow service_role access

### Optimization shows no improvement
â†’ Check training data quality (need diverse examples)
â†’ Ensure human feedback is present in metadata
â†’ May need more training examples (>20)

---

## Summary

You now have:
- âœ… PolicyAgent loading optimized prompts from database
- âœ… Automatic prompt optimization using GPT-4
- âœ… Performance tracking and metrics
- âœ… Zero-downtime prompt updates
- âœ… Fallback safety net
- âœ… Complete audit trail

**The optimization loop is closed!** ğŸ‰

Changes to prompts are now:
1. Data-driven (based on real failures)
2. Automated (GPT-4 does the analysis)
3. Trackable (all metrics in database)
4. Deployable (no code changes needed)
5. Reversible (can roll back instantly)

**Go ahead and run your first optimization!** ğŸš€

